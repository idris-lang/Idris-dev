\newcommand{\ttinterp}[1]{\mathcal{E}\interp{#1}}

\section{Elaborating \Idris{}}

\label{sect:elaboration}

An \Idris{} program consists of a series of declarations --- data types,
functions, type classes and instances. In this section, we describe how these
high level declarations are translated into a \TT{} program consisting of
inductive families and pattern matching function definitions. We will need to
work at the \remph{declaration} level, and at the \remph{expression} level,
defining the following meta-operations.

\begin{itemize}
\item $\ttinterp{\cdot}$, which builds a \TT{} expression from an \Idris{} expression.
\item $\MO{Elab}$, which processes a top level \Idris{} declaration by generating
one or more \TT{} declarations.
\item $\MO{TTDecl}$, which adds a top level \TT{} declaration.
\end{itemize}


\subsection{The Development Calculus \TTdev}

We build \TT{} expressions by using high level \Idris{} expressions to
direct a tactic based theorem prover, which builds the \TT{} expressions
step by step, by refinement. In order to build expressions in this way,
the type theory needs to support
\remph{incomplete} terms, and a method for term construction. 
To achieve this, we extend \TT{} with \remph{holes},
calling the extended calculus \TTdev{}.
Holes stand for the parts of programs which have not yet been
instantiated; this largely follows the \Oleg{} development
calculus~\cite{McBride1999}.

The basic idea is to extend the syntax for binders with a \remph{hole}
binding and a \remph{guess} binding. 
These extensions are given in Figure \ref{ttdev}.
The \remph{guess} binding is
similar to a $\LET$ binding, but without any computational force,
i.e. there are no reduction rules for guess bindings. 
Using binders to represent holes is useful in a dependently typed setting since
one value may determine another. Attaching a guess to a binder ensures that
instantiating one such value also instantiates all of its dependencies. The
typing rules for binders ensure that no $?$ bindings leak into types.

\FFIG{
\AR{
\vb ::= \ldots 
 \:\mid\: \hole{\vx}{\vt} \:\:(\mbox{hole binding}) \:\:
 \:\mid\: \guess{\vx}{\vt}{\vt} \:\:(\mbox{guess})
\medskip\\
\Rule{
\Gamma;\hole{\vx}{\vS}\proves\ve\Hab\vT
}
{
\Gamma\proves\hole{\vx}{\vS}\SC\ve\Hab\vT
}
\hspace*{0.1cm}\vx\not\in\vT
\hspace*{0.1in}\mathsf{Hole}
\hg
\Rule{
\Gamma;\guess{\vx}{\vS}{\ve_1}\proves\ve_2\Hab\vT
}
{
\Gamma\proves\guess{\vx}{\vS}{\ve_1}\SC\ve_2\Hab\vT
}
\hspace*{0.1cm}\vx\not\in\vT
\hspace*{0.1in}\mathsf{Guess}
}
}
{\TTdev{} extensions}
{ttdev}


\subsection{Proof State}

\label{sect:prfstate}

A proof state is a tuple, $(\vC, \Delta, \ve, \vQ)$, containing:

\begin{itemize}
\item A global context, $\vC$, containing pattern matching definitions and their types
\item A local context, $\Delta$, containing pattern bindings
\item A proof term, $\ve$, in \TTdev{}
\item A hole queue, $\vQ$
%\item \remph{Deferred} definitions, $\vD$, for introducing global metavariables
\end{itemize}

The \remph{hole queue} is a priority queue of names of hole and guess binders 
$\langle\vx_1,\vx_2,\ldots,\vx_n\rangle$
in the proof term ---
ensuring that each bound name is unique. Holes refer to \remph{sub goals}
in the proof.
When this queue is empty, the proof term is complete.
Creating a \TT{} expression from an \Idris{} expression involves creating
a new proof state, with an empty proof term, and using the high level definition
to direct the building of a final proof state, with a complete proof term.

In the implementation, the proof state is captured in an elaboration monad,
\texttt{Elab}, which includes various operations for querying and updating
the proof state, manipulating terms, generating fresh names, etc. However, we will
describe \Idris{} elaboration in terms of meta-operations on the proof state,
in order to capture the essence of the elaboration process without being distracted
by implementation details. These meta-operations include: 

\begin{itemize}
\item \demph{Queries} which retrieve values from the proof state, without modifying
the state. For example, we can:
\begin{itemize}
\item Get the type of the current sub goal ($\MO{Type}$)
\item Retrieve the local context $\Gamma$ at the current sub goal ($\MO{Context}$)
\item Type check ($\MO{Check}$) or normalise ($\MO{Normalise}$) a term relative to $\Gamma$
\end{itemize}
\item \demph{Unification}, which unifies two terms (potentially solving sub goals) 
relative to $\Gamma$
\item \demph{Tactics} which update the proof term. Tactics operate on the sub term
at the binder specified by the head of the hole queue $\vQ$.
\item \demph{Focussing} on a specific sub goal, which brings a different sub goal to the
head of the hole queue.
%\item \demph{Deferring} a sub goal, which adds a new definition to the global context
%$\vC$ which solves the sub goal.
\end{itemize}

Elaboration of an \Idris{} expression involves creating a new proof state, running
a series of tactics to build a complete proof term, then retrieving and \remph{rechecking}
the final proof term, which must be a \TT{} program (i.e. does not contain any of the
\TTdev{} extensions). We call a sub-term which contains no hole or guess bindings 
\demph{pure}. Although a pure term does not contain hole or guess bindings, it may
nevertheless \remph{refer} to hole- or guess-bound variables.

We initialise a proof state with the $\MO{NewProof}$ operation. Given a global
context $\vC$, $\MO{NewProof}\:\vt$ sets up the proof state as:

\DM{
(\vC, \cdot, \hole{\vx}{\vt}\SC\vx, \langle\vx\rangle)
}

The local context is initially empty, and the initial hole queue is the $\vx$ standing for
the entire expression. We can reset the proof term with the $\MO{NewTerm}$ operation.
In an existing proof state $(\vC, \Delta, \ve, \vQ)$,
$\MO{NewTerm}\:\vt$ discards the proof term and hole queue, and
updates the proof state to:

\DM{
(\vC, \Delta, \hole{\vx}{\vt}\SC\vx, \langle\vx\rangle)
}


This allows us in particular to use pattern bindings from the left hand side of a pattern
matching definition in the term on the right hand side.

\subsection{System State}

\label{sect:sysstate}

The system state is a tuple, $(\vC,\vA,\vI)$, containing:

\begin{itemize}
\item A global context, $\vC$, containing pattern matching definitions and their types
\item Implicit arguments, $\vA$, recording which arguments are implicit for each global name
\item Type class instances, $\vI$, containing dictionaries for type classes
\end{itemize}

In the implementation, the system state is captured in a monad, \texttt{Idris}, 
which also wraps the current proof state. This monad 
includes additional information such as syntax overloadings,
command line options, and optimisations, which do not concern us here. Elaboration
of expressions requires access to the system state in particular in order to expand
implicit arguments and resolve type classes. 

For each global name, $\vA$ records whether its arguments are explicit, implicit,
or type class constraints.  For example, recall the declaration
of \texttt{vAdd}:

\begin{SaveVerbatim}{vAddImpT}

vAdd : Num a => Vect a n -> Vect a n -> Vect a n

\end{SaveVerbatim}
\useverb{vAddImpT} 

\noindent
Written in full, and giving each argument an explicit name, we get the
type declaration:

\begin{SaveVerbatim}{vAddImpT}

vAdd : (a : _) -> (n : _) -> (c : Num a) -> 
       (xs : Vect a n) -> (ys : Vect a n) -> Vect a n

\end{SaveVerbatim}
\useverb{vAddImpT} 

\noindent
For \tFN{vAdd}, we record that \texttt{a} and \texttt{n} are implicit, 
\texttt{c} is a constraint, and \texttt{xs} and \texttt{ys} are explicit. When
the elaborator encounters an application of \tFN{vAdd}, it knows that unless these arguments
are given explicitly, the application must be expanded.

\newcommand{\Check}{\MO{Check}_\Gamma}
\newcommand{\Eval}{\MO{Normalise}_\Gamma}
\newcommand{\Unify}{\MO{Unify}_\Gamma}

\subsection{Tactics}

% Meta-operations Check, Normalise, Unify 
In order to build \TT{} expressions from \Idris{} programs, we define a collection
of meta-operations for querying and modifying the proof state. Meta-operations
may have side-effects including failure, or updating the proof state. We have the following
primitive meta-operations:

\begin{itemize}
\item $\MO{Focus}\:\vn$, which moves $\vn$ to the head of the hole queue.
\item $\MO{Unfocus}$, which moves the current hole to the back of the hole queue.
\item $\Check\:\ve$, which type checks an expression $\ve$ relative to a context
$\Gamma$, returning its type.
$\MO{Check}$ will fail
if the expression is not well-typed.
\item $\Eval\:\ve$, which evaluates a well-typed expression $\ve$ relative to a context 
$\Gamma$, returning its normal form.
\item 
$\Unify\:\ve_1\:\ve_2$, 
which unifies $\ve_1$ and $\ve_2$ by finding the values with which holes must be instantiated
for $\ve_1$ and $\ve_2$ to be convertible relative to $\Gamma$
(i.e. for $\Gamma\proves\ve_1\converts\ve_2$ to hold). $\MO{Unify}$ will fail
if it cannot find such values. If successful, $\MO{Unify}$ will update the proof state.
\end{itemize}

\demph{Tactics} are meta-operations which operate on the sub-term given
by the hole at the head of the hole queue in the proof state. They take the following form:

\DM{
\PA{\A\A}{
\MO{Tactic}_\Gamma & \:\vec{\VV{args}} & \:\vt & \MoRet{\vt'}
}
}

A tactic takes a sequence of zero or more arguments $\vec{\VV{args}}$ followed
by the sub-term $\vt$ on which it is operating. It runs relative to a context
$\Gamma$ which contains all the bindings and pattern bindings in scope at that
point in the term. The sub-term $\vt$ will either be a hole binding
$\hole{\vx}{\vT}\SC\ve$ or a guess binding $\guess{\vx}{\vT}{\vv}\SC\ve$. The
tactic returns a new term $\vt'$ which can take any form, provided it is
well-typed, with a type convertible to the type of $\vt$. 
Tactics may also have the side effect of updating the proof state,
therefore we will describe tactics in a pseudo-code with $\RW{do}$ notation.

We define a set of primitive tactics: $\MO{Claim}$, $\MO{Fill}$ and $\MO{Solve}$
which are used to create and destroy holes; and $\MO{Lambda}$, $\MO{Pi}$, $\MO{Let}$
and $\MO{Attack}$ which are used to create binders.

\subsubsection{Creating and destroying holes}

The $\MO{Claim}$ tactic, given a name and a type, adds a new hole binding in
the scope of the current goal $\vx$, adding the new binding to the hole queue, but
keeping $\vx$ at the head:

\DM{
\PA{\A\A}{
\MO{Claim}_\Gamma & (\vy \Hab\vS) & (\hole{\vx}{\vT}\SC\ve) & 
   \MoRet{\RW{return}\:\hole{\vx}{\vT}\SC\hole{\vy}{\vS}\SC\ve} \\
}
}

An obvious difficulty is in ensuring that names are unique throughout a proof term.
In the implementation, we ensure that any hole created by the $\MO{Claim}$ tactic
has a unique name by checking against existing names in scope and modifying
the name if necessary. In this paper, we will assume that all created names are fresh.

The $\MO{Fill}$ tactic, given a value $\vv$, attempts to solve the current goal
with $\vv$, creating a guess binding in its place. $\MO{Fill}$ attempts to
solve other holes by unifying the expected type of $\vx$ with the type of $\vv$:

\DM{
\PA{\A\A}{
\MO{Fill}_\Gamma & \vv & (\hole{\vx}{\vT}\SC\ve) & 
   \MoRet{\RW{do}\:\AR{
   \vT' \leftarrow \Check\:\vv\\
   \Unify\:\vT\:\vT'\\
   \RW{return}\:\guess{\vx}{\vT}{\vv'}\SC\ve}
   } \\
}
}

\noindent
For example, consider the following proof term:

\DM{
\AR{
\hole{\vA}{\Set}\SC\hole{\vk}{\Nat}\SC
\hole{\vx}{\vA}\SC\hole{\vxs}{\Vect\:\vA\:\vk}\SC
\\
\hole{\vys}{\Vect\:\vA\:(\suc\:\vk)}\SC\vys
}
}

\noindent
If $\vx$ is in focus (i.e., at the head of the hole queue) and we attempt to
$\MO{Fill}$ it with an $\TC{Int}$ value $42$, we have:

\begin{itemize}
\item $\Check\:42\:\mq\:\TC{Int}$
\item Unifying $\TC{Int}$ with $\vA$ (the type of $\vx$) is only possible if
$\vA\:=\:\TC{Int}$, so we solve $\vA$.
\end{itemize}

\noindent
Therefore the resulting proof term is:

\DM{
\AR{
\hole{\vk}{\Nat}\SC
\guess{\vx}{\TC{Int}}{42}\SC\hole{\vxs}{\Vect\:\TC{Int}\:\vk}\SC
\\
\hole{\vys}{\Vect\:\TC{Int}\:(\suc\:\vk)}\SC\vys
}
}

The $\MO{Solve}$ tactic operates on a guess binding. If the guess is \remph{pure}, i.e., it
is a \TT{} term containing no hole or guess bindings, then the value attached to
the guess is substituted into its scope:

\DM{
\PA{\A}{
\MO{Solve}_\Gamma & (\guess{\vx}{\vT}{\vv}\SC\ve) &
   \MoRet{\RW{return}\:\ve[\vv/\vx]\hg\mbox{(if $\MO{Pure}\:\vv$)}}
}
}

In each of these tactics, if any step fails, or the term in focus does not take
the correct form (e.g. is not a guess in the case of $\MO{Solve}$ or not a hole
in the case of $\MO{Claim}$ and $\MO{Fill}$, the entire tactic fails. We can
handle failure using the $\MO{Try}$ tactic combinator:

\DM{
\PA{\A\A\A}{
\MO{Try}_\Gamma & \VV{t1} & \VV{t2} & \vt &
   \MoRet{\AR{\VV{t1}_\Gamma\:\vt\hg\mbox{(if $\VV{t1}$ succeeds)} \\
              \VV{t2}_\Gamma\:\vt\hg\mbox{(otherwise)}}}
}
}

We have a primitive tactic $\MO{Fail}$, which may be invoked by
any tactic which encounters a failure condition (for example, an unsolvable unification
problem) and is handled by $\MO{Try}$.

\subsubsection{Creating binders}

We also define primitive tactics for constructing binders. We can create a $\lambda$
binding if the goal normalises to a function type:

\DM{
\PA{\A\A}{
\MO{Lambda}_\Gamma & \vn & (\hole{\vx}{\vT}\SC\vx) &
 \MoRet{\RW{do}\:\AR{
   \all{\vy}{\vS}\SC\vT'\:\leftarrow\:\Eval\:\vT \\
   \RW{return}\:\lam{\vn}{\vS}\SC\hole{\vx}{\vT'[\vn/\vy]}\SC\vx
   }
   }
}
}

\noindent
We can create a $\forall$ binding if the goal is a $\TC{Set}$:

\DM{
\PA{\A\A}{
\MO{Pi}_\Gamma & (\vn\Hab\vS) & (\hole{\vx}{\Set}\SC\vx) &
 \MoRet{\RW{do}\:\AR{
   \Set\:\leftarrow\:\Check\:\vS\\
   \RW{return}\:\all{\vn}{\vS}\SC\hole{\vx}{\Set}\SC\vx
   }
   }
}
}

\noindent
To create a $\LET$ binding, we give a type and a value.

\DM{
\PA{\A\A}{
\MO{Let}_\Gamma & (\vn\Hab\vS\defq\vv) & (\hole{\vx}{\vT}\SC\vx) &
 \MoRet{\RW{do}\:\AR{
   \Set\:\leftarrow\:\Check\:\vS\\
   \vS'\:\leftarrow\:\Check\:\vv\\
   \Unify\:\vS\:\vS'\\
   \RW{return}\:\LET\:\vn\Hab\vS\defq\vv\SC\hole{\vx}{\vT}\SC\vx
   }
   }
}
}

Each of these tactics require the term in focus to be of the form $\hole{\vx}{\vT}\SC\vx$.
This is important, because if the scope of the binding was an arbitrary expression $\ve$,
the binder would be scoped across this whole expression rather than the subexpression
$\vx$ as intended.
The $\MO{attack}$ tactic ensures that a hole
is in the appropriate form, creating a new hole $\vh$ which is placed at the head
of the queue:

\DM{
\PA{\A}{
\MO{Attack}_\Gamma & (\hole{\vx}{\vT}\SC\ve) &
 \MoRet{\RW{return}\:\guess{\vx}{\vT}{(\hole{\vh}{\vT}\SC\vh)}\SC\ve}
}
}

Finally, we can convert a hole binding to a pattern binding by giving the 
pattern variable a name. This solves a hole
by adding the pattern binding to the proof state, and updating the proof term
with the pattern variable directly:

\DM{
\PA{\A\A}{
\MO{Pat}_\Gamma & \vn & (\hole{\vx}{\vT}\SC\ve) &
  \MoRet{\RW{do}\:\AR{
    \MO{PatBind}\:(\vx\Hab\vT)\\
    \RW{return}\:\ve[\vn/\vx]
  }}
}
}

The $\MO{PatBind}$ operation simply updates the proof state with the given pattern
binding. Once we have created bindings from the left hand side of a pattern
matching definition, for example, we can retain these bindings for use when
building the right hand side.

\subsubsection{Example}

Tactics are executed by a higher level meta-operation $\MO{RunTac}$, which
locates the appropriate sub-term, applies the tactic with the context
local to this sub-term, and
replaces the sub-term with the term returned by the
tactic. It then updates the hole queue in the proof state, and updates holes which have
been solved by unification. If the tactic creates new holes, these are automatically
added to the \remph{head} of the hole queue.
For example, consider the following simple
\TT{} definition for the identify function:

\DM{
\AR{
\FN{id}\Hab\all{\vA}{\Set}\SC\all{\va}{\vA}\SC\vA \\
\FN{id}\:=\:\lam{\vA}{\Set}\SC\lam{\va}{\vA}\SC\va  
}
}

We can build $\FN{id}$ either as a complete term, or by applying a sequence of tactics.
To achieve this, we create a proof state initialised with the type of $\FN{id}$ and
apply a series of $\MO{Lambda}$ and $\MO{Fill}$ operations using $\MO{RunTac}$.
Note that the types on each $\MO{Lambda}$ are solved by unification:

\DM{
\AR{
\MO{MkId}\:\mq\:\RW{do}\:
 \AR{
   \MO{NewProof}\:\all{\vA}{\Set}\SC\all{\va}{\vA}\SC\vA \\
   \MO{RunTac}\;\MO{Attack} \\
   \MO{RunTac}\;(\MO{Lambda}\;\vA) \\
   \MO{RunTac}\;\MO{Attack} \\
   \MO{RunTac}\;(\MO{Lambda}\;\va) \\
   \MO{RunTac}\;(\MO{Fill}\;\va)\\ 
   \MO{RunTac}\;\MO{Solve}\\
   \MO{RunTac}\;\MO{Solve}\\
   \MO{RunTac}\;\MO{Solve}\\
   \MO{Term}
 }
}
}

To aid readability, we will elide $\MO{RunTac}$, and use a semi-colon to indicate
sequencing --- in the implementation we use wrapper functions for each tactic to
apply $\MO{RunTac}$.
Using this convention, we can build $\FN{id}$'s type and definition as shown
in Figure \ref{idelab}. Note that $\MO{Term}$ retrieves the proof term from the current proof
state. Both $\MO{MkIdType}$ and $\MO{MkId}$ finish by returning a completed \TT{} term.
Note in particular that each $\MO{Attack}$ and each $\MO{Fill}$, which create new guesses,
are closed with a $\MO{Solve}$.

Setting up elaboration in this way, with a proof state captured in a monad,
and a primitive collection of tactics,
makes it easy to derive more complex tactics for elaborating higher level language constructs,
in much the same way as the \texttt{Ltac} language in Coq. As a result, the
description of elaboration of
a language construct (or a program such as $\FN{id}$) bears a strong resemblance to
a Coq proof script.

\FFIG{
\AR{
\MO{MkIdType}\:\mq\:\RW{do}\;
 \AR{
   \MO{NewProof}\:\Set\\
   \MO{Attack} ; \MO{Pi}\:(\vA\Hab\Set) ;
   \MO{Attack} ; \MO{Pi}\:(\va\Hab\vA) \\
   \MO{Fill}\;\vA \\
   \MO{Solve} ; \MO{Solve} ; \MO{Solve} \\
   \MO{Term}
 }
 \medskip\\
\MO{MkId}\:\mq\:\RW{do}\;
 \AR{
   \vt\:\leftarrow\:\MO{MkIdType}; \MO{NewProof}\:\vt\\
   \MO{Attack} ; \MO{Lambda}\;\vA ; 
   \MO{Attack} ; \MO{Lambda}\;\va \\
   \MO{Fill}\;\va \\
   \MO{Solve} ; \MO{Solve} ; \MO{Solve}\\
   \MO{Term}
 }
}
}
{Building $\FN{id}$ with tactics}
{idelab}

%--- give unify in full, esp. as it solves sub goals? Maybe...

% Unify' G x t             = Success (x, t) if ?x : t in G
% Unify' G t x             = Success (x, t) if ?x : t in G
% Unify' G (b x. e) (b' x'. e')   = Unify' G b b'; Unify' G;b e e'[x/x']
% Unify' G ((\x.e) x) e'   = Unify' G e e' 
% Unify' G e ((\x.e') x)   = Unify' G e e' 
% Unify' G (f es) (f' es') = vs <- Unify' G f f'; Injective f
                             
% Unify' G x y             = Success () if G |- x == y
% Unify' G . .             = Failure

% Unify' G (\x : t . e) (\x : t' . e') = Unify' G t t'; Unify' G e e'
% ...


%\DM{
%}

\subsection{Elaborating Expressions}

\newcommand{\piimp}[2]{\mbox{\texttt{\{ $#1$ : $#2$ \} -> }}}
%\mathtt{\{}#1\mathtt{:}#2\mbox{\texttt{\} -> }}}
\newcommand{\piexp}[2]{\mbox{\texttt{( $#1$ : $#2$ ) -> }}}
%\newcommand{\piexp}[2]{\mathtt{(}#1\mathtt{:}#2\mbox{\texttt{) -> }}}
\newcommand{\piconst}[1]{\mbox{\texttt{$#1$ => }}}
\newcommand{\icase}{\mathtt{case}}
\newcommand{\iwith}{\mathtt{with}}
\newcommand{\idata}{\mathtt{data}}
\newcommand{\iclass}{\mathtt{class}}
\newcommand{\iinstance}{\mathtt{instance}}
\newcommand{\iwhere}{\mathtt{where}}
\newcommand{\iof}{\mathtt{of}}
\newcommand{\ilet}[2]{\mathtt{let}\;#1\;\mathtt{=}\;#2\;\mathtt{in}}
\newcommand{\ilam}[1]{\mathtt{\backslash}\;#1\;\mathtt{=>}}
\newcommand{\iarg}[2]{\mbox{\texttt{\{$#1$ = $#2$\}}}}
\newcommand{\ihab}[2]{\mbox{\texttt{$#1$ : $#2$}}}
\newcommand{\carg}[1]{\mbox{\texttt{\{\{$#1$\}\}}}}
\newcommand{\fatarrow}{\mbox{\texttt{=>}}}
\newcommand{\ibar}{\mbox{\texttt{|}}}
\newcommand{\mvar}[1]{\mbox{\texttt{?}}#1}

\FFIG{
\AR{
\begin{array}{rll@{\hg}rll}
\ve ::= & \vc & (\mbox{constant}) &
\mid & \vx & (\mbox{variable}) \\
\mid & \vt & (\mbox{type}) &
\mid & \ve\:\ta & (\mbox{function application}) \\
\mid & \ilam{\vx}\:\ve & (\mbox{lambda abstraction}) &
\mid & \ilet{\vx}{\ve}\:\ve & (\mbox{let binding}) \\
%\mid & \icase\:\ve\:\iof\:\vec{\VV{alt}} & (\mbox{case expression}) &
%\mid & \mvar{\vx} & (\mbox{metavariable}) \\
\mid & \_ & (\mbox{placeholder})
\end{array}
\medskip\\
\begin{array}{rll}
\va :: = & \ve & (\mbox{normal argument}) \\
\mid & \iarg{\vx}{\ve} & (\mbox{implicit argument with value}) \\
\mid & \carg{\ve} & (\mbox{explicit class instance}) 
\end{array}
\medskip\\
\begin{array}{rll}
\vt ::= & \ve & (\mbox{expression}) \\
\mid & \piimp{\vx}{\vt}\vt & (\mbox{implicit function space}) \\
\mid & \piexp{\vx}{\vt}\vt & (\mbox{explicit function space}) \\
\mid & \VV{constr}\;\vt & (\mbox{constrained type}) \\
\end{array}
\medskip\\
\begin{array}{rll}
\VV{constr} ::= & \piconst{\ttt} & (\mbox{type class constraint}) \\
\end{array}
}
}
{\IdrisM{} expressions}
{idrism}

\FFIG{
\AR{
\begin{array}{rll}
\vd ::= & \ihab{\vx}{\vt} & (\mbox{type declaration}) \\
\mid & \VV{pclause} & (\mbox{pattern clause}) \\
\mid & \VV{ddecl} & (\mbox{data type declaration}) \\
\mid & \VV{cdecl} & (\mbox{class declaration}) \\
\mid & \VV{idecl} & (\mbox{instance declaration}) \\
\end{array}
\medskip\\
\begin{array}{ll}
\begin{array}{rll}
\VV{pclause} ::= & 
\vx\:\ttt\:[\ibar\;\te]\mbox{\texttt{ = }}\ve \hg[\iwhere\;\td]\\ 
\mid & \AR{
\vx\:\ttt\:[\ibar\;\te]\;\iwith\;\ve\\
\hg\vec{\VV{pclause}}
}
\end{array}
&
\begin{array}{rll}
\VV{ddecl} ::= & \idata\;\ihab{\vx}{\vt}\;\iwhere\;\vec{\VV{con}}\\
\VV{con} ::= & \ihab{\vx}{\vt}\\
\medskip\\
\VV{cdecl} ::= & \iclass\;[\VV{constr}]\;\vx\;(\ihab{\tx}{\ttt})\;\iwhere\;\td
\\
\VV{idecl} ::= & \iinstance\:[\VV{constr}]\;\vx\;\ttt\;\iwhere\;\td
\end{array}
\end{array}
}
}
{\IdrisM{} declarations}
{idrismd}

We define a language \IdrisM{}, with expression syntax given in Figure \ref{idrism}
and declaration syntax given in Figure \ref{idrismd}.
\IdrisM{} is a subset of \Idris{} without syntactic sugar --- that is, without
tuple syntax, \texttt{do}-notation or infix operators --- and with implicit
arguments in types bound explicitly 
(e.g. $\piimp{\va}{\_}\piimp{\vn}{\_}\TC{Vect}\:\va\:\vn$
instead of simply $\Vect\:\va\:\vn$).
It is in general straightforward to
convert full \Idris{} to \IdrisM{} --- syntactic sugar is implemented by a
direct source transformation, and implicit arguments can be identified as the names
which are free in a type in a non-function position.
%
\IdrisM{} differs from \TT{} in several important respects. It has implicit
syntax and type classes, and functions are applied to multiple arguments rather
than one at a time. 

%Extensions over \TT{}: implicit syntax, case expressions. Functions are applied to
%multiple arguments, rather than one at a time (this helps with implicit syntax).
%\IdrisM{} also supports explicit \demph{metavariables}. A metavariable \texttt{?mvar}
%creates a new global function \tFN{mvar}, with its type such that it would
%be type correct to apply \tFN{mvar} to all of the variables in scope.

%Implicit and type class arguments? Expanded at the application site (we need to know
%it's the global name after all and we do that by type).

To elaborate expressions, we define a meta-operation $\ttinterp{\cdot}$ which
runs relative to a proof state (see Section \ref{sect:prfstate}).  Its effect is
to update the proof state so that the hole in focus contains a representation
of the given expression, by applying tactics. We assume that the proof state
has already been set up, which means that elaboration can always be
\remph{type-directed} since the proof state contains the type of the expression we
are building.

\subsubsection{Elaborating variables and constants}

In the simplest cases, there is a direct translation from an \IdrisM{} expression to
a \TT{} expression --- we build \TT{} representations of variables and constants using
the \MO{Fill} tactic:

\DM{
\AR{
\ttinterp{\vx}\:\mq\:\RW{do}\:\MO{Fill}\:\vx;\:\MO{Solve}\\
\ttinterp{\vc}\:\mq\:\RW{do}\:\MO{Fill}\:\vc;\:\MO{Solve}\\
}
}

We need not concern ourselves with type checking variables or constants here --- $\MO{Fill}$
will handle this, type checking $\vx$ or $\vc$ and unifying the result with the
hole type. If there are any errors, elaboration will fail.

If we are building the left hand side of a pattern clause, however, there is a problem,
as it is the left hand side which \remph{defines} variables. In this context, we assume
that attempting to elaborate a variable which does not type check means that the variable
is a pattern variable:

\DM{
\ttinterp{\vx}\:\mq\:
\MO{Try}\:\AR{(\RW{do}\:\MO{Fill}\:\vx;\:\MO{Solve})\\
(\MO{Pat}\:\vx)\hg\hg\mbox{(If elaborating a left hand side)}}
}

We also need to elaborate \remph{placeholders}, which are subexpressions we expect to
solve by unification. In this case, we simply move on to the next hole in the queue, moving
the current hole to the end of the queue with $\MO{Unfocus}$:

\DM{
\ttinterp{\_}\:\mq\:\MO{Unfocus}
}

On encountering a placeholder, our assumption is that unification will eventually solve
the hole. At the end of elaboration, any holes remaining unsolved on the left hand side
become pattern variables. If there are any unsolved on the right hand side, elaboration
fails.

\subsubsection{Elaborating bindings}

To elaborate a $\lambda$-binding, we $\MO{Attack}$ the hole, which must be a function type,
apply the $\MO{Lambda}$ tactic, elaborate the scope, then $\MO{Solve}$, which discharges
the $\MO{Attack}$:

\DM{
\ttinterp{\ilam{\vx}\ve}\:\mq\:
\RW{do}\:
\AR{
\MO{Attack};\;\MO{Lambda}\:\vx\\
\ttinterp{\ve}\\
\MO{Solve}
}
}

Note that there is no type on the $\lambda$-binding in \IdrisM{}. There is no need --- since
elaboration is type directed, the $\MO{Lambda}$ tactic finds the type of the binding by
looking at the type of the hole. In full \Idris{}, types are allowed on bindings, and the
elaborator merely checks that the given type is equivalent to the inferred type.

Elaborating a function type is more tricky, since we have to elaborate the argument
type (itself an \IdrisM{} expression) then elaborate the scope. To achieve this, we
create a new goal $\vX$ for the argument type $\vt$, where $\vX$ is a fresh name,
and introduce a function binding with argument type $\vX$. We can then focus on
$\vX$, and elaborate it with the expression $\vt$. Finally, we elaborate the
scope of the binding.

\DM{
\AR{
\ttinterp{\piexp{\vx}{\vt}\ve}\:\mq\:
\RW{do}\:
\AR{
\MO{Attack};\;\MO{Claim}\:(\vX\Hab\Set)\\
\MO{Pi}\:(\vx\Hab\vX)\\
\MO{Focus}\:\vX\\
\ttinterp{\vt};\;
\ttinterp{\ve}\\
\MO{Solve}
}
}
}

Elaborating $\vt$ will involve solving unification problems, which will, if the
program is type correct, solve $\vX$. After focussing on $\vX$ and elaborating
$\vt$, there is no need to refocus on the hole representing the scope, as it was
previously at the head of the hole queue before focussing on $\vX$.
Elaboration of implicit and constraint
argument types is exactly the same --- \TT{} makes no distinction between then.

To elaborate a \texttt{let} binding, we take a similar approach, creating new subgoals
for the \texttt{let}-bound value and its type, then elaborating the scope. Again,
if elaborating the scope is successful, unification will solve the claimed variables
$\vV$ and $\vX$.

\DM{
\ttinterp{\ilet{\vx}{\vv}\:{\ve}}\:\mq\:\RW{do}\:
\AR{
\MO{Attack};\;
\MO{Claim}\:(\vX\Hab\Set);\; 
\MO{Claim}\:(\vV\Hab\vX)\\
\MO{Let}\:(\vx\Hab\vX\defq\vV)\\
\MO{Focus}\:\vV\\
\ttinterp{\vv};\;
\ttinterp{\ve}
}
}
\subsubsection{Elaborating applications}

There are two cases to consider when elaborating applications:

\begin{itemize}
\item Applying a global function name to arguments, some of which may be implicit.
\item Applying an expression, which does not have implicit arguments, to an argument.
\end{itemize}

In the first case, we must expand the application to include implicit arguments.
For example \texttt{vAdd xs ys} is expanded to
\texttt{vAdd \{a=\_\} \{n=\_\} \{\{\_\}\} xs ys}, adding the implicit arguments
\texttt{a} and \texttt{n} and a type class instance argument. The meta-operation
$\MO{Expand}\:\vx\:\ta$, given a global function name $\vx$ and the
arguments supplied by the programmer $\ta$, returns a new argument list
$\ta'$ with implicit and type class arguments added. Each value in
$\ta'$ is paired with an explicit name for the argument.

Implicit arguments are solved by unification
--- the type or value of another argument determines the value of an implicit argument,
with the appeal to $\MO{Unify}$ in the $\MO{Fill}$ tactic solving as many extra holes
as it can. However, unification problems can take several forms. For example, assuming
$\vf$, $\vg$, $\vx$ are holes, we might have unification problems of the following
forms:

\DM{
\AR{
\MO{Unify}_\Gamma\;\vf\:\TC{Int}\\
\MO{Unify}_\Gamma\;(\vf\:\vx)\:\TC{Int}\\
\MO{Unify}_\Gamma\;(\vf\:\vx)\:(\vg\:\vx)\\
}
}

The first problem has a solution: $\vf=\TC{Int}$. The second and third problems have
no solution without further information. In the second case, we cannot conclude 
anything about $\vf$ or $\vx$ just from knowing that $\vf\:\vx\:=\:\TC{Int}$.
In the third case, although we have $\vx\:=\:\vx$ in argument position, we cannot
conclude that $\vf\:=\:\vg$ unless we know that the function which solves $\vf$ or
$\vg$ is injective.

Once $\vf$ or $\vg$ is solved in some other way, perhaps by being given explicitly
in another argument, these unification problems can make progress. Otherwise, unification
fails. As a result, the order in which functions and arguments are
elaborated matters. Sometimes, we may learn more by elaborating a function first
and the arguments later, sometimes it may be the other way around. 

One option when unification fails is to store the problem (i.e. the expressions to be
unified with their local context) and refine it with further information when it becomes 
available. In practice, we have found a simpler approach to be effective: try
elaborating the function first, then arguments. If that fails, try elaborating the
arguments first.

Elaborating a global function application makes a $\MO{Claim}$ for each argument
in turn. Then the function is elaborated, applied to each of the claimed arguments.
Finally, each argument which has been given explicitly is elaborated. If this fails,
elaboration tries again in the opposite order. Finally, we resolve any type
class instance arguments with the built-in $\MO{Instance}$ tactic (see Section
\ref{sect:instance}).

\DM{
\ttinterp{\vx\:\ta}\:\mq\:
\AR{
\RW{do}\:
\AR{
(\tn,\tv)\:\gets\:\MO{Expand}\:\vx\:\ta\\
\vec{\MO{Claim}}\:(\tn\Hab\tT)\\
\MO{Try}\:\AR{
(\RW{do}\:\AR{
\MO{Fill}\:\vx\:\tn\\
\vec{\MO{Focus}}\:\tn;\:\ttinterp{\tv}\hg\mbox{(for non-placeholder $\vv$)}\\
\MO{Solve})
}\\
(\RW{do}\:\AR{
\vec{\MO{Focus}}\:\tn;\:\ttinterp{\tv}\hg\mbox{(for non-placeholder $\vv$)}\\
\MO{Fill}\:\vx\:\tn\\
\MO{Solve})
}}
\\
\vec{\MO{Instance}}\:\tn\hg\mbox{(for type class constraint argument $\vn$)}
}
}
}

The $\MO{Instance}$ tactic focuses on the given hole and searches the context for
a type class instance which would solve the goal directly. First, it examines the local
context, then recursively searches the global context. $\MO{Instance}$ is covered
in detail in Section \ref{sect:instance}.

To elaborate a simple function application, of an arbitrary expression to an arbitrary
argument, we need not worry about implicit arguments or class constraints. Instead,
we simply elaborate the function and argument, and apply the results. Since elaboration
is type directed, however, we do need to construct an appropriate type for the function.
As before, we try elaboration in two orders, as we may learn something from elaborating
the function or argument which helps with unification:

\DM{
\ttinterp{\ve\:\va}\:\mq\:\RW{do}\:
\AR{
\MO{Claim}\:(\vA\Hab\Set);\;\MO{Claim}\:(\vB\Hab\Set)\\
\MO{Claim}\:(\vf\Hab\vA\to\vB);\;\MO{Claim}\:(\vs\Hab\vA)\\
\MO{Try}\:
\AR{
(\RW{do}\:\AR{
\MO{Focus}\:\vf;\;\ttinterp{\ve}\\
\MO{Focus}\:\vs;\;\ttinterp{\va})}\\
(\RW{do}\:\AR{
\MO{Focus}\:\vs;\;\ttinterp{\va}\\
\MO{Focus}\:\vf;\;\ttinterp{\ve})}\\
}
}
}

%\subsubsection{Elaborating metavariables}

%\subsubsection{Elaborating \texttt{case} expressions}

\subsection{Elaborating Declarations}

To elaborate declarations, we define a meta-operation $\MO{Elab}$, which runs
relative to the system state (see Section \ref{sect:sysstate}) and has access to the
current proof state. $\MO{Elab}$ uses $\ttinterp{\cdot}$ to help translate
\IdrisM{} type, function and class declarations into \TT{} declarations.

\newcommand{\edo}[1]{\RW{do}\:\AR{#1}}

\subsubsection{Elaborating Type Declarations}

To elaborate a type declaration, we simply create a new proof state,
translate the \IdrisM{} type to a \TT{} type, then add the resulting type
as a \TT{} declaration:

\DM{
\MO{Elab}\:(\vx\Hab\vt)\:\mq
\:\edo{\MO{NewProof}\:\Set\\
       \ttinterp{\vt}\\
       \vt'\gets\MO{Term}\\
       \MO{TTDecl}\:(\vx\Hab\vt')
}}

The final $\MO{TTDecl}$ takes the result of elaboration, type checks it,
and adds it to the global context if type checking succeeds. This final type check
ensures that the elaboration process does not allow any ill-typed terms to creep
into the context.

\subsubsection{Elaborating Data Types}

To elaborate a data declaration, we need to elaborate the type declaration itself,
as a normal type declaration. This ensures that the type is in scope when
elaborating the constructor types. We then use the results to build a \TT{} data
declaration.

\DM{
\AR{
\MO{Elab}\:(\idata\;\ihab{\vx}{\vt}\;\iwhere\;\tc)\\
\hg\hg\mq\:\edo{\MO{NewProof}\:\Set\\
                \ttinterp{\vt}\\
                \vt'\gets\MO{Term}\\
                \MO{TTDecl}\:(\vx\Hab\vt')\\
                \tc'\gets\vec{\MO{ElabCon}}\:\tc\\
                \MO{TTDecl}\:(\Data\:\vx\Hab\vt'\:\Where\:\tc')
}
\AR{
\MO{ElabCon}\:(\vx\Hab\vt)\\
\hg\hg\mq\: \edo{\MO{NewProof}\:\Set\\
                 \ttinterp{\vt}\\
                 \vt'\gets\MO{Term}\\
                 \RW{return}\:(\vx\Hab\vt')
}
}}}

\subsubsection{Elaborating Pattern Matching}

To elaborate a pattern matching definition, we work clause by clause, elaborating
the left and right hand sides in turn. $\MO{ElabClause}$ returns the elaborated
left and right hand sides, and may have the side effect of adding entries
to the global context, such as definitions in $\iwhere$ clauses.
First, let us consider the simplest case, with no $\iwhere$ clause:

\DM{
\MO{ElabClause}\:(\vx\:\ttt\:=\:\ve)\:\mq\:?
}

How do we elaborate the left hand side, given that elaboration is type directed, and
we do not know its type until after we have elaborated it? 
We can achieve this without any change to the elaborator by defining 
a type $\TC{Infer}$:

\DM{
\Data\hg\TC{Infer}\Hab\Set\hg\Where\hg\DC{MkInfer}\Hab\all{\va}{\Set}\SC\va\to\TC{Infer}
}

Now to elaborate the left hand side, we elaborate $\DC{MkInfer}\:\_\:(\vx\:\ttt)$ and
extract the value and unified type when complete.

\DM{
\AR{
\MO{ElabClause}\:(\vx\:\ttt\:=\:\ve)\:\mq\:\\
\hg\hg\edo{
\MO{NewProof}\:\TC{Infer};\;
\ttinterp{\DC{MkInfer}\:\_\:(\vx\:\ttt)}\\
\DC{MkInfer}\:\vT\:\VV{lhs}\gets\MO{Term}\\
\tp\gets\MO{Patterns}\\
\MO{NewTerm}\:\vT;\;
\ttinterp{\ve}\\
\VV{rhs}\gets\MO{Term}\\
\RW{return}\:(\RW{var}\:\tp\SC\VV{lhs}\:=\:\VV{rhs})
}
}
}

This infers a type for the left hand side, creating pattern variable bindings. Then, 
it elaborates the right hand side using the inferred type of the left hand side
and the inferred pattern bindings, which are retrieved from the state with the
$\MO{Patterns}$ operation. Finally, it returns a pattern clause in \TT{} form.
To elaborate a collection of pattern clauses, we simply map $\MO{ElabClause}$ over
the clauses and add the resulting collection to \TT{}.

\DM{
\AR{
\MO{Elab}\:\vec{\VV{pclause}}\:\mq\:
\edo{
\tc\gets\vec{\MO{ElabClause}}\:\vec{\VV{pclause}}\\
\vec{\MO{TTDecl}}\:\tc
}
}
}

Elaborating a clause is made only slightly more complex by the presence of
$\iwhere$ clauses. In this case, we elaborate the left hand side, add the pattern
bound variables as extra arguments to the declarations in the $\iwhere$ block,
and recursively elaborate before elaborating the right hand side.

\DM{
\AR{
\MO{ElabClause}\:(\vx\:\ttt\:=\:\ve\:\iwhere\:\td)\:\mq\:\\
\hg\hg\edo{
\MO{NewProof}\:\TC{Infer};\;
\ttinterp{\DC{MkInfer}\:\_\:(\vx\:\ttt)}\\
\DC{MkInfer}\:\vT\:\VV{lhs}\gets\MO{Term}\\
\tp\gets\MO{Patterns}\\
(\td', \ve') \gets\MO{Lift}\:\tp\:\td\:\ve\\
\vec{\MO{Elab}}\:\td'\\
\MO{NewTerm}\:\vT;\;
\ttinterp{\ve'}\\
\VV{rhs}\gets\MO{Term}\\
\RW{return}\:(\RW{var}\:\tp\SC\VV{lhs}\:=\:\VV{rhs})
}
}
}

In \TT{}, all definitions must be at the top level. Therefore, the declarations 
in the $\iwhere$ block are lifted out, adding the pattern bound names as additional arguments
to ensure they are in scope, using the $\MO{Lift}$ operation. This also modifies
the right hand side $\ve$ to use the lifted definitions, rather than the original.

\subsubsection{Elaborating the \texttt{with} rule}

The \texttt{with} rule allows \remph{dependent} pattern matching on intermediate values.
To translate this into \TT{}, we need to construct an auxiliary top level
definition for 
the intermediate pattern match. For example, recall \tFN{natToBin}:

\useverb{natToBin}

\noindent
This is equivalent to the following, using top level definitions only:

\begin{SaveVerbatim}{natToBinw}

natToBin : Nat -> List Bool
natToBin O = Nil
natToBin k = natToBin' k (parity k)

\end{SaveVerbatim}
\begin{SaveVerbatim}{natToBinwp}

natToBin' : (n : Nat) -> Parity n -> List Bool
natToBin' (j + j)     even = False :: natToBin j
natToBin' (S (j + j)) odd  = True  :: natToBin j

\end{SaveVerbatim}
\useverb{natToBinw}

\useverb{natToBinwp}

The difference when using the \texttt{with} rule is that there is no need for
the explicit type declaration for the auxiliary function \texttt{natToBin}.
Elaboration of \texttt{with} builds the type declaration and auxiliary function
automatically:


\DM{
\AR{
\MO{ElabClause}\:(\vx\:\ttt\:\iwith\:\ve\:\vec{\VV{pclause}})\:\mq\\
\hg\hg\edo{
\MO{NewProof}\:\TC{Infer};\;
\ttinterp{\DC{MkInfer}\:\_\:(\vx\:\ttt)}\\
\DC{MkInfer}\:\vT\:\VV{lhs}\gets\MO{Term}\\
\tp\gets\MO{Patterns}\\
\MO{NewTerm}\:\TC{Infer};\;
\ttinterp{\DC{MkInfer}\:\_\:\ve}\\
\DC{MkInfer}\:\vW\:\vw'\gets\MO{Term}\\
\MO{TTDecl}\:(\vx'\Hab\tp\to\vW\to\vT)\\
\tc'\gets\vec{\MO{MatchWith}}\:\tp\:(\vx\:\ttt)\:\vec{\VV{pclause}}\\
\vec{\MO{Elab}}\:\tc'\\
\RW{return}\:(\RW{var}\:\tp\SC\VV{lhs}\:=\:\vx'\:\tp\:\vw')
}
\medskip\\
\MO{MatchWith}\:\tp\:(\vx\:\ttt)\:(\vx\:\tw\:\mid\:\ve\:\VV{rhs})\:\mq\\
\hg\hg\edo{
\tp'\gets\vec{\MO{Match}}\:\tw\:\ttt\\
\MO{CheckComplete}\:\tp\:\tp'\\
\RW{return}\:(\vx'\:\tp'\:\ve\:\VV{rhs})
}
}
}

A $\iwith$ block in a clause for $\vx$
results in an auxiliary definition $\vx'$, where $\vx'$ is a fresh name,
which takes an extra argument corresponding to the intermediate result which
is to be matched.
$\MO{MatchWith}$ matches the left hand side of the top level definition against
each $\VV{pclause}$, using the result of the match to build each pattern clause
for $\vx'$. 
The clauses in the with block must be of a specific form: they must have an initial
set of arguments $\tw$ which matches the outer clause arguments
$\ttt$, and an extra argument
$\ve$ which is the same type as the scrutinee of the \texttt{with}, $\vW$.
The right hand side, $\VV{rhs}$, may either return a value directly, containing 
\texttt{where} clauses, or be a nested \texttt{with} block. In any case, it
remains unchanged.
$\MO{Match}\:\vw\:\vt$ returns a list of pattern bindings containing
the variables in $\vt$ and their matches in $\vw$.
If the \texttt{with} block is well-formed, then
the resulting set of patterns $\tp'$ contains exactly the same pattern variables
as $\tp$, which is verified by $\MO{CheckComplete}$.


\subsubsection{Elaborating Class and Instance Declarations}

Type classes are implemented as dictionaries of functions for each instance. Therefore,
a type class declaration elaborates to a record type containing all of the functions
in the class, and an instance is simply an instance of the record. The methods
are translated to top level functions which extract the relevant function from the 
dictionary.  
For example, for the \texttt{Show} type class, we translate the \texttt{class}
declaration to:

\begin{SaveVerbatim}{showdata}

data Show : Set -> Set where
    instanceShow : (show : a -> String) -> Show a

show : Show a -> a -> String
show (instanceShow show') = show'

\end{SaveVerbatim}
\useverb{showdata} 

\noindent
An instance for a type \texttt{a} is then a value of type \texttt{Show a}; for example for Nat:

\begin{SaveVerbatim}{shownatf}

showNat : Show Nat;
showNat = instanceShow show' where
    show' : Nat -> String
    show' O = "O"
    show' (S k) = "s" ++ show k

\end{SaveVerbatim}
\useverb{shownatf} 

The call to \tFN{show} in the body of the instance uses the top level \tFN{show}
function, with the type class instance resolved by the elaborator.
Where instances have constraints, we can simply pass these constraints on to
the function declaration for the instance. For example, for \texttt{Show (List a)}:

\begin{SaveVerbatim}{showlista}

showList : Show a => Show (List a)
showList = instanceShow show' where
    show' : List a -> String
    show' []        = "[]"
    show' (x :: xs) = show x ++ " :: " ++ show xs

\end{SaveVerbatim}
\useverb{showlista} 

\noindent
Where classes have constraints, we can again pass the
constraints on to the data type declaration, and leave resolving the
constraints to the elaborator. For example, for \texttt{Ord}:

\begin{SaveVerbatim}{eqorda}

class Eq a => Ord a where
    compare : a -> a -> Ordering
    max : a -> a -> a
    -- remaining methods elided

\end{SaveVerbatim}
\useverb{eqorda}

\noindent
This translates to:

\begin{SaveVerbatim}{orddata}

data Ord : Set -> Set where
    instanceOrd : Eq a => (compare : a -> a -> Ordering) -> 
                          (max : a -> a -> a) ->  Ord a 

compare : Ord a -> a -> a -> Ordering
compare (instanceOrd compare' max') = compare'

max : Ord a -> a -> a -> Ordering
max (instanceOrd compare' max') = max'

\end{SaveVerbatim}
\useverb{orddata}

We also add functions to retrieve parent classes from a dictionary, to assist
with type class resolution. In this case, we add:

\begin{SaveVerbatim}{ordparent}

OrdEq : Ord a -> Eq a
OrdEq (instanceOrd {{eq}} compare' max') = eq

\end{SaveVerbatim}
\useverb{ordparent} 

\noindent
In general, we elaborate \texttt{class} declarations as follows:

\DM{
\AR{
\MO{Elab}\:(\iclass\;\piconst{\tc}\TC{C}\:(\ta\Hab\ttt)\:\iwhere\:\td)
\\
\hg\hg
\mq\:\RW{do}\:\AR{
(\vec{\VV{meth}}\Hab\ttt)\gets\td\\
\MO{Elab}\:
\AR{(\RW{data}\:\TC{C}\Hab(\ta\Hab\ttt)\to\Set\:\iwhere\\
\hg\DC{InstanceC}\Hab\piconst{\tc}\td\to\TC{C}\:\ta)}
}
}
}

\DM{
\AR{
\AR{
\vec{\MO{ElabMeth}}\:\vec{\VV{meth}}\:\td\\
\vec{\MO{ElabParent}}\:\tc
}
\medskip\\
\MO{ElabMeth}\:(\vf\Hab\vt)\\
\hg\hg\mq\:\RW{do}\:\AR{
\MO{Elab}\:(\vf\Hab\TC{C}\:\ta\to\vt)\\
\MO{Elab}\:(\vf\:(\DC{InstanceC}\:\vec{\VV{meth}})\:=\:\vec{\VV{meth}[\vf]})
}
\medskip\\
\MO{ElabParent}\:\vc\\
\hg\hg\mq\:\RW{do}\:\AR{
\MO{Elab}\:(\VV{cC}\Hab\TC{C}\:\ta\to\vc)\\
\MO{Elab}\:(\VV{cC}\:(\DC{InstanceC}\:\carg{\vc'}\:\vec{\VV{meth}})\:=\:\vc')
}
}
}

\noindent
Then we elaborate \texttt{instance} declarations as follows:

\DM{
\AR{
\MO{Elab}\:(\iinstance\;\piconst{\tc}\TC{C}\:\ttt\:\iwhere\:\td)\\
\hg\hg\mq\:\RW{do}\:\AR{
(\vec{\VV{meth}}, \vec{\VV{ty}})\gets\td\\
\vec{\VV{ty'}}\:=\:\vec{\VV{ty}}[\ttt/\ta]\\
\MO{Elab}\:(\FN{instanceC}\Hab\piconst{\tc}\TC{C}\:\ttt)\\
\MO{Elab}\:\AR{
(\FN{instanceC}\:=\:\DC{InstanceC}\;\vec{\VV{meth'}}\;\iwhere
\\
\hg\vec{\VV{meth'}}\Hab\vec{\VV{ty'}}\\
\hg\td
)}
}
}
}

Adding default definitions is straightforward: simply insert the default definition where
there is a method missing in an \texttt{instance} declaration.

\textbf{Remark:} Here, we have added a higher level language construct (type
classes) simply by elaborating in terms of lower level language constructs
(data types and functions).  We achieve type class resolution by implementing a
tactic, $\MO{Instance}$.  We can build several extensions this way because we
have effectively built, bottom up, an Embedded Domain Specific Language for
constructing programs in \TT{}.

\subsubsection{The $\MO{Instance}$ tactic}

\label{sect:instance}

When elaborating applications, recall that we fill in implicit arguments by
unification, and type class constraints by an $\MO{Instance}$ tactic which searches
the context for an instance of the required type class. 

The $\MO{Instance}$ tactic begins by trying to find a solution in the local context,
using the $\MO{Trivial}$ tactic, which attempts
to solve the current goal with each local variable in turn. It is used as a very
basic primitive for proof search, and is defined as follows:

\DM{
\AR{
\MO{Trivial}\:\mq\:
\RW{do}\:\AR{
      (\tv\Hab\ttt)\gets\MO{Context}\\
      \MO{TryAll}\:\tv
    }
\medskip\\
\MO{TryAll}\:(\vv_1,\vv_2\ldots)\:\mq\:\MO{Try}\:(\MO{Fill}\:\vv_1)\:(\MO{TryAll}\:\vv_2\ldots)
\\
\MO{TryAll}\:\langle\rangle\:\mq\:\MO{Fail}
}
}

For example, in the following function, we resolve the type class constraint required
for the $\texttt{>}$ operator by finding an instance for \texttt{Ord a} in the local
context:

\begin{SaveVerbatim}{ordlocal}

max : Ord a => a -> a -> a
max x y = if (x > y) then x else y

\end{SaveVerbatim}
\useverb{ordlocal} 

If $\MO{Trivial}$ does not find a solution, $\MO{Instance}$ begins a global search,
applying every type class instance, and attempting to resolve the instance's parameters
recursively. Given an instance $\vI$ with parameters $\ta$:

\DM{
\MO{TryInstance}\:(\vI\:\ta)\:\mq\:\RW{do}\;
\AR{
\vec{\MO{Claim}}\:(\tx\Hab\ta)\\
\ttinterp{\vI\:\tx}\\
\vec{\MO{Focus}}\:\tx;\;\vec{\MO{Instance}}
}
}

We define $\MO{Instance}$ as follows, using $\MO{AllInstances}$ to retrieve
all possible type class instances in the form required by $\MO{TryInstance}$:

\DM{
\MO{Instance}\:\mq\:
\MO{Try}\:
\AR{
\MO{Trivial}\\
(\RW{do}\:\AR{
\ti\gets\MO{AllInstances}\\
\vec{\MO{TryInstance}}\:\ti
)
}
}
}

In practice, for efficiency and to ensure that type class resolution terminates,
we constrain the search by recording which instances are defined for each type class,
and by ensuring that a recursive call of $\MO{Instance}$ is either searching for
a different class, or a structurally smaller instance of the current class. 

For example, in the following function, we resolve the constraints required by the 
\tFN{show} function by a global search which locates a \tTC{Show} instance for
\tTC{List a}, which in turn requires a \tTC{Show} instance for \texttt{a}.
In this case \texttt{a} is instantiated by \tTC{Int}, so we finish by locating
a \tTC{Show} instance for \tTC{Int}:

\begin{SaveVerbatim}{showinst}

main : IO ()
main = putStrLn (show [1,2,3,4]) 

\end{SaveVerbatim}
\useverb{showinst} 

\subsection{Syntax Extensions}

We now have a complete elaborator for \IdrisM{}, covering 
dependent pattern matching definitions and expansion of implicit arguments
and type classes. In order to make the language truly general purpose, however, we
will need to add higher level extensions. Full \Idris{} is \IdrisM{} extended
with \texttt{do}-notation, idiom brackets~\cite{McBride2007}, \texttt{case} expressions,
pattern matching \texttt{let}, metavariables and tactic based theorem proving.
The majority of these extensions are straightforward transformations of high level
\Idris{} programs --- for example, \texttt{do}-notation can be reduced directly to
\IdrisM{} function applications. Some, however, require further support. In this section,
we complete the presentation of \IdrisM{} by extending it with metavariables
and $\icase$ expressions:

\DM{
\AR{
\begin{array}{rll@{\hg}rll}
\ve ::= & \ldots \\
\mid & \mvar{\vx} & (\mbox{metavariable}) &
\mid & \icase\:\ve\:\iof\:\vec{\VV{alt}} & (\mbox{case expression}) \\
\medskip\\
\VV{alt} ::= & \ve\:\fatarrow\:\ve & (\mbox{case alternative})\\
\end{array}
}
}

\subsubsection{Metavariables}

\demph{Metavariables} are terms which stand for incomplete programs. A
metavariable serves a similar
purpose to a hole binding in \TT{}, but gives rise to a global rather than a
local variable. For example, the following is an incomplete definition of
the vector append function, containing a metavariable \texttt{append\_rec}:

\begin{SaveVerbatim}{vappmv}

(++) : Vect A n -> Vect A m -> Vect A (n + m)
(++) Nil       ys = ys
(++) (x :: xs) ys = x :: ?append_rec

\end{SaveVerbatim}
\useverb{vappmv} 

This generates a new (as yet undefined)
function \texttt{append\_rec}, which takes all of the variables
in scope at the point it is used, and returns a value of the required type.
We can inspect the type of \texttt{append\_rec} at the \Idris{} prompt:

\begin{SaveVerbatim}{appendrec}

*vec> :t append_rec
append_rec : (a : Set) -> (m : Nat) -> (n : Nat) -> a -> 
             Vect a n -> Vect a m -> Vect a (n + m)

\end{SaveVerbatim}
\useverb{appendrec} 

Metavariables serve two purposes: Firstly, they aid type directed program development
by allowing a programmer direct access to the inferred types of local variables, and
the types of subexpressions. Secondly, they allow a separation of program structure
from proof details. Metavariables can be solved either by directly editing program
source, or by providing a definition elsewhere in the file. For example, we can later say:

\begin{SaveVerbatim}{appendrec_def}

append_rec a m n x xs ys = app xs ys

\end{SaveVerbatim}
\useverb{appendrec_def} 

To elaborate a metavariable, we simply add a new top level definition, and apply
it to the current context:

\DM{
\ttinterp{\mvar{\vx}}\:\mq\:
\edo{
(\tv\Hab\ttt)\gets\MO{Context}\\
\vT\gets\MO{Type}\\
\MO{TTDecl}\:(\vx\Hab(\tv\Hab\ttt)\to\vT)\\
\ttinterp{\vx\:\tv}
}
}

\subsubsection{\texttt{case} expressions}

A \texttt{case} expression allows pattern matching on intermediate values. The difficulty
in elaborating \texttt{case} expressions is that \TT{} allows matching only on
\emph{top level} values. To elaborate a \texttt{case} expression, therefore,
we create a new top level function standing for the expression, and apply it. The natural
way to implement this is to use a metavariable. For example, we have already seen
\texttt{lookup\_default}:

\useverb{listlookup}

\noindent
This elaborates as follows:

\begin{SaveVerbatim}{listlookupmv}

lookup_default : Nat -> List a -> a -> a
lookup_default i xs def = 
   let scrutinee = list_lookup i xs in ?lookup_default_case

lookup_default_case i xs def Nothing  = def
lookup_default_case i xs def (Just x) = x

\end{SaveVerbatim}
\useverb{listlookupmv} 

To elaborate a $\icase$ expression, we begin by \texttt{let} binding the scrutinee of
the $\icase$ expression and creating a metavariable $\VV{xcase}$ to implement the
pattern matching. We then build a new pattern matching definition for $\VV{xcase}$
and elaborate it:

\DM{
\AR{
\ttinterp{\icase\:\ve\:\iof\:\vec{\VV{alt}}}\:\mq\:
\edo{
\ttinterp{\ilet{\VV{scrutinee}}{\ve}\:\mvar{\VV{xcase}}}\\
(\tv\Hab\ttt)\gets\MO{Context}\\
\vec{\MO{MkCase}}\:\tv\:\VV{alt}\\
}
\medskip\\
\MO{MkCase}\:\tv\:(\vl\:\fatarrow\:\vr)\:\mq\:\MO{Elab}\:(\VV{xcase}\:\tv\:\vl\:=\:\vr)
}
}

Any nested $\icase$ expressions will be handled by the recursive call to
$\MO{Elab}$. Again, because of the way we have set up an Embedded Domain Specific
Language for describing elaboration, we have been able to implement a new higher
level language feature in terms of elaboration of lower level language features.

%\subsubsection{Tactic-implicit arguments}


%\subsubsection{Pairs and Dependent Pairs}

% Other extensions: do notation, idiom brackets, pairs, etc, are easily expressed
% by transformations of the high level syntax

